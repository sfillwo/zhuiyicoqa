# coding: utf-8
import json
from collections import OrderedDict, Counter
from tqdm import tqdm
import string
import spacy
import multiprocessing
import re
import os

from config import args
from logger import config_logger

logger = config_logger("coqa_utils", "INFO")

class CoQAReader():
    def __init__(self, history, question_sep=" Q: ", answer_sep=" A: ", evaluator=None):
        self.tokenizer = RobertaTokenizer()
        self.history = history  # prepend dialog history. -1 represent all previous dialog
        self.question_sep = question_sep
        self.answer_sep = answer_sep
        self.evaluator = evaluator

    def read(self, file_path, data_type):
        if data_type not in ["train", "dev", "test"]:
            raise ValueError()
        logger.info("Reading file at %s", file_path)
        logger.info("Processing the dataset.")
        instances = self._read(file_path, data_type)
        return instances

    def _read(self, source, data_type):
        with open(source, 'r', encoding='utf-8') as f:
            source_data = json.load(f)

        storys = []
        questions = []
        answers = []
        indexs = [0]
        for i, dialog in enumerate(source_data["data"]):
            storys.append(dialog["story"])
            for question in dialog["questions"]:
                questions.append(question["input_text"])
            for answer, answer1, answer2, answer3 in zip(dialog["answers"], dialog["additional_answers"]["0"], dialog["additional_answers"]["1"], dialog["additional_answers"]["2"]):
                answers.append(answer["input_text"] + ' ' + answer1["input_text"] + ' ' + answer2["input_text"] + ' ' + answer3["input_text"])
            indexs.append(indexs[-1] + len(dialog["questions"]))

        all_story_tokens, all_story_token_spans = self.tokenizer.word_tokenizer_parallel(storys)
        all_question_tokens, _ = self.tokenizer.word_tokenizer_parallel(questions)
        all_answer_tokens, _ = self.tokenizer.word_tokenizer_parallel(answers)

        i = 0
        instances = []
        for dialog, story_tokens, story_token_spans in tqdm(
                zip(source_data["data"], all_story_tokens, all_story_token_spans), desc='Processing...'):
            s = indexs[i]
            e = indexs[i + 1]
            # if data_type == "train":
            self.process_dialog(dialog, story_tokens, story_token_spans, all_answer_tokens[s:e])
            self._make_instances(instances, dialog, story_tokens, story_token_spans, all_question_tokens[s:e],
                                 all_answer_tokens[s:e], data_type)
            i += 1
        return instances

    def process_dialog(self, dialog, story_tokens, story_token_spans, answer_tokens):
        story = dialog["story"]
        story_id = dialog["id"]
        questions = dialog["questions"]
        answers = dialog["answers"]
        additional_answers = dialog["additional_answers"]
        if len(answer_tokens) != len(answers):
            raise ValueError(
                "Answer token turns does match answer number: {} {}\n".format(len(answer_tokens), len(answers)))

        for i, (answer, answer1, answer2, answer3) in enumerate(zip(answers, additional_answers["0"], additional_answers["1"], additional_answers["2"])):
            if i + 1 != answer["turn_id"]:
                raise ValueError("Question turn id does match answer: {} {}\n".format(story_id, answer["turn_id"]))
            answer_type = self.get_answer_type([answer["input_text"], answer1["input_text"], answer2["input_text"], answer3["input_text"]])
            answer["answer_type"] = answer_type
            # self.find_extractive_span(story, story_tokens, story_token_spans, answer, answer_tokens[i], answer_type)
            self.find_extractive_span(story, story_id, story_tokens, story_token_spans, answer, answer1, answer2, answer3, answer_tokens[i], answer_type)

    def get_answer_type(self, answers):
        answer_type = "extractive"
        type_count = {}
        for ans in answers:
            norm_ans = CoQAEvaluator.normalize_answer(ans)
            if norm_ans in type_count:
                type_count[norm_ans] += 1
            else:
                type_count[norm_ans] = 1
        for ans in type_count:
            if type_count[ans] > 1:
                if ans in ["unknown", "yes", "no"]:
                    answer_type = ans
        return answer_type

    def find_extractive_span(self, story, story_id, story_tokens, story_token_spans, answer, answer1, answer2, answer3, answer_tokens, answer_type):

        if answer_type != "unknown":
            # rationale text is ground_truth or evidence
            rationale = answer["span_text"]
            norm_rationale = rationale.strip().strip("'").strip()
            idx = rationale.find(norm_rationale)
            start = min(answer["span_start"], answer1["span_start"], answer2["span_start"], answer3["span_start"]) + idx
            end = max(answer["span_end"], answer1["span_end"], answer2["span_end"], answer3["span_end"]) + idx
            word_ixs = self.get_word_span(story_token_spans, start, end)
            answer["rationale"] = story[story_token_spans[word_ixs[0]][0]: story_token_spans[word_ixs[-1]][1]]
            answer["rationale_char_start"] = story_token_spans[word_ixs[0]][0]
            answer["rationale_char_end"] = story_token_spans[word_ixs[-1]][1]
            answer["rationale_start"] = word_ixs[0]
            answer["rationale_end"] = word_ixs[-1]

        if answer_type != "extractive":
            return

        best_f1 = 0.0
        ground_truth = []
        for w in answer_tokens:
            w = CoQAEvaluator.normalize_answer(w)
            if w != "":
                ground_truth.append(w)

        if not ground_truth:
            answer["best_f1_answer"] = ""  # input_text is not extractive in context
            answer["answer_type"] = "skip"
            return

        rationale_tokens = [CoQAEvaluator.normalize_answer(story_tokens[i]) for i in word_ixs]
        ls = [i for i in range(len(rationale_tokens))
              if rationale_tokens[i] in ground_truth]

        # cal best f1 between evidence and ground_truth
        for i in range(len(ls)):
            for j in range(i, len(ls)):
                pred = []
                k = ls[i]
                while k <= ls[j]:
                    if rationale_tokens[k] != "":
                        pred.append(rationale_tokens[k])
                    k += 1
                if not pred:
                    continue
                f1 = self.evaluator.compute_turn_score(story_id, answer['turn_id'], ' '.join(pred))['f1']
                if f1 > best_f1:
                    best_f1 = f1
                    best_span = (word_ixs[ls[i]], word_ixs[ls[j]])

        if best_f1 > 0.:
            i, j = best_span
            start = story_token_spans[i][0]
            end = story_token_spans[j][1]
            answer["best_f1_answer_char_start"] = start
            answer["best_f1_answer_char_end"] = end
            answer["best_f1_answer_start"] = i
            answer["best_f1_answer_end"] = j
            answer["best_f1_answer"] = story[start:end]
            answer["best_f1"] = best_f1
        else:
            # search full passage
            tokens_norm = [CoQAEvaluator.normalize_answer(w) for w in story_tokens]
            ls = [i for i in range(len(tokens_norm)) if tokens_norm[i] in ground_truth]
            for i in range(len(ls)):
                for j in range(i, len(ls)):
                    pred = []
                    k = ls[i]
                    while k <= ls[j]:
                        if tokens_norm[k] != "":
                            pred.append(tokens_norm[k])
                        k += 1
                    f1 = self.evaluator.compute_turn_score(story_id, answer['turn_id'], ' '.join(pred))['f1']
                    if f1 > best_f1:
                        best_f1 = f1
                        best_span = (ls[i], ls[j])

            if best_f1 > 0.:
                i, j = best_span
                start = story_token_spans[i][0]
                end = story_token_spans[j][1]
                answer["best_f1_answer_char_start"] = start
                answer["best_f1_answer_char_end"] = end
                answer["best_f1_answer_start"] = i
                answer["best_f1_answer_end"] = j
                answer["best_f1_answer"] = story[start:end]
                answer["best_f1"] = best_f1

                answer["rationale"] = story[start:end]
                answer["rationale_char_start"] = start
                answer["rationale_char_end"] = end
                answer["rationale_start"] = i
                answer["rationale_end"] = j
            else:
                answer["best_f1_answer"] = ""  # input_text in not extractive in context
                answer["answer_type"] = "skip"
        return

    def get_word_span(self, spans, start, end):
        idxs = []
        for word_ix, (s, e) in enumerate(spans):
            if e > start:
                if s < end:
                    idxs.append(word_ix)
                else:
                    break
        return idxs

    def _make_instances(self, instances, dialog, story_tokens, story_token_spans, question_tokens, answer_tokens,
                        data_type):

        if len(dialog["questions"]) != len(question_tokens):
            raise ValueError(
                "Question tokens turns does match question turn number: {} {}\n".format(len(question_tokens),
                                                                                        len(dialog["questions"])))
        if len(dialog["answers"]) != len(answer_tokens):
            raise ValueError("Answer tokens turns does match answer turn number: {} {}\n".format(len(answer_tokens),
                                                                                                 len(dialog[
                                                                                                         "answers"])))

        story_id = dialog["id"]
        story = dialog["story"]

        arr = []
        input_answers = []
        skips = [False] * len(dialog["answers"])
        answer_types_id = {"unknown": 0, "yes": 1, "no": 2, "extractive": 3, "skip": 0}
        for idx, answer in enumerate(dialog["answers"]):
            input_answers.append(answer["input_text"])
            answer_type = answer["answer_type"]
            instance = OrderedDict({})
            arr.append(instance)
            instance["answer"] = answer["input_text"]
            instance["answer_start"] = 0
            instance["answer_end"] = 0
            answer_type_one_hot = [0, 0, 0]
            if answer_type != "extractive":
                answer_type_id = answer_types_id[answer_type]
                answer_type_one_hot[answer_type_id] = 1
            instance["abstractive_answer_mask"] = answer_type_one_hot
            instance["answer_type"] = answer_type
            if answer_type == "skip":
                instance["answer_type"] = "unknown"

            instance["rationale"] = None
            instance["rationale_start"] = 0
            instance["rationale_end"] = 0
            if data_type == "train" and answer_type == "skip":
                skips[idx] = True
                # continue
            if answer_type == "unknown":
                instance["rationale"] = "unknown"
                instance["rationale_start"] = 0
                instance["rationale_end"] = 0
            else:
                if "rationale" in answer:
                    instance["rationale"] = answer["rationale"]
                    instance["rationale_start"] = answer["rationale_start"]
                    instance["rationale_end"] = answer["rationale_end"]
                # else:
                #     raise ValueError(story_id, idx + 1, "no rationale")

            if "best_f1_answer_start" in answer:
                instance["answer"] = answer["best_f1_answer"]
                instance["answer_start"] = answer["best_f1_answer_start"]
                instance["answer_end"] = answer["best_f1_answer_end"]
            elif answer_type == "extractive":
                raise ValueError(story_id, idx + 1, "no ext ans")
            
            if answer_type in ['yes', 'no', 'unknown']:
                instance['fake_answer'] = answer_type
            else:
                instance['fake_answer'] = answer["best_f1_answer"]
                if instance['fake_answer'] in ['to','with', 'they','them','he', 'she', 'it', 'is',
                     'are', 'was', 'were', 'very', 'here', 'there', 'their','this','that','so',
                     's','over', 'or', 'and','to be', 'she is', 'he is', 'it is','she was', 
                     'he was', 'on', 'in', 'of', 'not', 'new', 'near', 'mrs', 'mr', 'most', 
                     'more','make', 'late', 'last', 'just','its','it\'s','it is', 'it was',
                     'it on','it off when','it is too', 'it at','into','onto','if','her','his',
                     'him','get','got','from','for','by','best','because','be','at','as','all',
                     'about','not','times']:
                    instance['fake_answer'] = ""

        questions = dialog["questions"]
        questions_with_history, question_tokens_with_history = self.get_concat_questions(questions, question_tokens,
                                                                                         input_answers, answer_tokens,
                                                                                         skips)

        idx = 0
        while idx < len(questions_with_history):
            if data_type == "train" and skips[idx]:
                idx += 1
                continue
            question = questions_with_history[idx]
            instance = arr[idx]
            instance["context"] = story
            instance["context_tokens"] = story_tokens
            instance["context_token_spans"] = story_token_spans
            instance["question"] = questions_with_history[idx]
            instance["question_tokens"] = question_tokens_with_history[idx]
            instance["qid"] = story_id + '|' + str(dialog["questions"][idx]["turn_id"])
            instance["context_word_len"] = [len(word) for word in instance["context_tokens"]]
            instance["question_word_len"] = [len(word) for word in instance["question_tokens"]]
            instances.append(instance)
            idx += 1

    def get_concat_questions(self, questions, question_tokens, answers, answer_tokens, skips):

        question_sep = self.question_sep
        answer_sep = self.answer_sep

        questions_with_history = []
        question_tokens_with_history = []
        i = 0
        while i < len(questions):
            start = 0
            if self.history >= 0:
                start = i - self.history
                if start < 0:
                    start = 0

            concat_ = ""
            concat_tokens = []
            while start < i:
                if not skips[start]:
                    concat_ += question_sep
                    concat_ += questions[start]["input_text"]
                    concat_ += answer_sep
                    concat_ += answers[start]

                    concat_tokens.append(question_sep)
                    concat_tokens += question_tokens[start]
                    concat_tokens.append(answer_sep)
                    concat_tokens += answer_tokens[start]
                start += 1
            concat_ += question_sep
            concat_tokens.append(question_sep)
            concat_ += questions[i]["input_text"]
            concat_tokens += question_tokens[i]

            questions_with_history.append(concat_)
            question_tokens_with_history.append(concat_tokens)
            i += 1

        return questions_with_history, question_tokens_with_history


# coqa offical evaluator
out_domain = ["reddit", "science"]
in_domain = ["mctest", "gutenberg", "race", "cnn", "wikipedia"]
domain_mappings = {"mctest": "children_stories", "gutenberg": "literature", "race": "mid-high_school", "cnn": "news",
                   "wikipedia": "wikipedia", "science": "science", "reddit": "reddit"}


class CoQAEvaluator():

    def __init__(self, gold_file, monitor="f1"):
        self.gold_data, self.id_to_source,self.id_to_type = CoQAEvaluator.gold_answers_to_dict(gold_file)
        self.monitor = monitor

    def get_monitor(self):
        return self.monitor

    @staticmethod
    def gold_answers_to_dict(gold_file):
        dataset = json.load(open(gold_file))
        gold_dict = {}
        id_to_source = {}
        id_to_type = {}
        for story in dataset['data']:
            source = story['source']
            story_id = story['id']
            id_to_source[story_id] = source
            questions = story['questions']
            multiple_answers = [story['answers']]
            multiple_answers += story['additional_answers'].values()
            for i, qa in enumerate(questions):
                qid = qa['turn_id']
                if i + 1 != qid:
                    sys.stderr.write("Turn id should match index {}: {}\n".format(i + 1, qa))
                gold_answers = []
                for answers in multiple_answers:
                    answer = answers[i]
                    if qid != answer['turn_id']:
                        sys.stderr.write("Question turn id does match answer: {} {}\n".format(qa, answer))
                    gold_answers.append(CoQAEvaluator.normalize_answer(answer['input_text']))
                key = (story_id, qid)
                if key in gold_dict:
                    sys.stderr.write("Gold file has duplicate stories: {}".format(source))
                gold_dict[key] = gold_answers
                if "yes" in gold_answers:
                    id_to_type[key] = "yes"
                elif "no" in gold_answers:
                    id_to_type[key] = "no"
                elif "unknown" in gold_answers:
                    id_to_type[key] = "unknown"
                else:
                    id_to_type[key] = "extractive"
        return gold_dict, id_to_source, id_to_type

    @staticmethod
    def preds_to_dict(pred_file):
        preds = json.load(open(pred_file))
        pred_dict = {}
        for pred in preds:
            pred_dict[(pred['id'], pred['turn_id'])] = pred['answer']
        return pred_dict

    @staticmethod
    def normalize_answer(s):
        """Lower text and remove punctuation, storys and extra whitespace."""

        def remove_articles(text):
            regex = re.compile(r'\b(a|an|the)\b', re.UNICODE)
            return re.sub(regex, ' ', text)

        def white_space_fix(text):
            return ' '.join(text.split())

        def remove_punc(text):
            exclude = set(string.punctuation)
            return ''.join(ch for ch in text if ch not in exclude)

        def lower(text):
            return text.lower()

        return white_space_fix(remove_articles(remove_punc(lower(s))))

    @staticmethod
    def get_tokens(s):
        if not s: return []
        return CoQAEvaluator.normalize_answer(s).split()

    @staticmethod
    def compute_exact(a_gold, a_pred):
        gold_toks = CoQAEvaluator.get_tokens(a_gold)
        pred_toks = CoQAEvaluator.get_tokens(a_pred)
        common = Counter(gold_toks) & Counter(pred_toks)
        num_same = sum(common.values())
        if len(gold_toks) == 0 or len(pred_toks) == 0:
            # If either is no-answer, then F1 is 1 if they agree, 0 otherwise
            return int(gold_toks == pred_toks)
        if num_same == 0:
            return 0
        recall = 1.0 * num_same / len(gold_toks)
        return recall

    @staticmethod
    def compute_f1(a_gold, a_pred):
        gold_toks = CoQAEvaluator.get_tokens(a_gold)
        pred_toks = CoQAEvaluator.get_tokens(a_pred)
        common = Counter(gold_toks) & Counter(pred_toks)
        num_same = sum(common.values())
        if len(gold_toks) == 0 or len(pred_toks) == 0:
            # If either is no-answer, then F1 is 1 if they agree, 0 otherwise
            return int(gold_toks == pred_toks)
        if num_same == 0:
            return 0
        precision = 1.0 * num_same / len(pred_toks)
        recall = 1.0 * num_same / len(gold_toks)
        f1 = (2 * precision * recall) / (precision + recall)
        return f1

    @staticmethod
    def _compute_turn_score(a_gold_list, a_pred):
        f1_sum = 0.0
        em_sum = 0.0
        if len(a_gold_list) > 1:
            for i in range(len(a_gold_list)):
                # exclude the current answer
                gold_answers = a_gold_list[0:i] + a_gold_list[i + 1:]
                em_sum += max(CoQAEvaluator.compute_exact(a, a_pred) for a in gold_answers)
                f1_sum += max(CoQAEvaluator.compute_f1(a, a_pred) for a in gold_answers)
        else:
            em_sum += max(CoQAEvaluator.compute_exact(a, a_pred) for a in a_gold_list)
            f1_sum += max(CoQAEvaluator.compute_f1(a, a_pred) for a in a_gold_list)

        return {"recall": em_sum / max(1, len(a_gold_list)), 'f1': f1_sum / max(1, len(a_gold_list))}

    def compute_turn_score(self, story_id, turn_id, a_pred):
        ''' This is the function what you are probably looking for. a_pred is the answer string your model predicted. '''
        key = (story_id, turn_id)
        a_gold_list = self.gold_data[key]
        return CoQAEvaluator._compute_turn_score(a_gold_list, a_pred)

    def get_raw_scores(self, pred_data):
        ''''Returns a dict with score with each turn prediction'''
        exact_scores = {}
        f1_scores = {}
        for story_id, turn_id in self.gold_data:
            key = (story_id, turn_id)
            if key not in pred_data:
                # sys.stderr.write('Missing prediction for {} and turn_id: {}\n'.format(story_id, turn_id))
                continue
            a_pred = pred_data[key]
            scores = self.compute_turn_score(story_id, turn_id, a_pred)
            # Take max over all gold answers
            exact_scores[key] = scores["recall"]
            f1_scores[key] = scores['f1']
        return exact_scores, f1_scores

    def get_raw_scores_human(self):
        ''''Returns a dict with score for each turn'''
        exact_scores = {}
        f1_scores = {}
        for story_id, turn_id in self.gold_data:
            key = (story_id, turn_id)
            f1_sum = 0.0
            em_sum = 0.0
            if len(self.gold_data[key]) > 1:
                for i in range(len(self.gold_data[key])):
                    # exclude the current answer
                    gold_answers = self.gold_data[key][0:i] + self.gold_data[key][i + 1:]
                    em_sum += max(CoQAEvaluator.compute_exact(a, self.gold_data[key][i]) for a in gold_answers)
                    f1_sum += max(CoQAEvaluator.compute_f1(a, self.gold_data[key][i]) for a in gold_answers)
            else:
                exit("Gold answers should be multiple: {}={}".format(key, self.gold_data[key]))
            exact_scores[key] = em_sum / len(self.gold_data[key])
            f1_scores[key] = f1_sum / len(self.gold_data[key])
        return exact_scores, f1_scores

    def human_performance(self):
        exact_scores, f1_scores = self.get_raw_scores_human()
        return self.get_domain_scores(exact_scores, f1_scores)

    def model_performance(self, pred_data):
        exact_scores, f1_scores = self.get_raw_scores(pred_data)
        return self.get_domain_scores(exact_scores, f1_scores)

    def get_score(self, pred_data):
        scores, type_scores = self.model_performance(pred_data)
        logger.info("{}".format(scores))
        logger.info("{}".format(type_scores))
        return scores['overall']

    def get_domain_scores(self, exact_scores, f1_scores):
        sources = {}
        type_results = {}
        for source in in_domain + out_domain:
            sources[source] = Counter()

        for tt in ["yes","no","unknown","extractive"]:
            type_results[tt] = Counter()

        for story_id, turn_id in self.gold_data:
            key = (story_id, turn_id)
            source = self.id_to_source[story_id]
            sources[source]['em_total'] += exact_scores.get(key, 0)
            sources[source]['f1_total'] += f1_scores.get(key, 0)
            sources[source]['turn_count'] += 1

            ans_type = self.id_to_type[key]
            type_results[ans_type]["em_total"] += exact_scores.get(key, 0)
            type_results[ans_type]['f1_total'] += f1_scores.get(key, 0)
            type_results[ans_type]['turn_count'] += 1

        scores = OrderedDict()
        in_domain_em_total = 0.0
        in_domain_f1_total = 0.0
        in_domain_turn_count = 0

        out_domain_em_total = 0.0
        out_domain_f1_total = 0.0
        out_domain_turn_count = 0

        for source in in_domain + out_domain:
            domain = domain_mappings[source]
            scores[domain] = {}
            scores[domain]["recall"] = round(sources[source]['em_total'] / max(1, sources[source]['turn_count']) * 100, 2)
            scores[domain]['f1'] = round(sources[source]['f1_total'] / max(1, sources[source]['turn_count']) * 100, 2)
            scores[domain]['turns'] = sources[source]['turn_count']
            if source in in_domain:
                in_domain_em_total += sources[source]['em_total']
                in_domain_f1_total += sources[source]['f1_total']
                in_domain_turn_count += sources[source]['turn_count']
            elif source in out_domain:
                out_domain_em_total += sources[source]['em_total']
                out_domain_f1_total += sources[source]['f1_total']
                out_domain_turn_count += sources[source]['turn_count']

        scores["in_domain"] = {"recall": round(in_domain_em_total / max(1, in_domain_turn_count) * 100, 2),
                               'f1': round(in_domain_f1_total / max(1, in_domain_turn_count) * 100, 2),
                               'turns': in_domain_turn_count}
        scores["out_domain"] = {"recall": round(out_domain_em_total / max(1, out_domain_turn_count) * 100, 2),
                                'f1': round(out_domain_f1_total / max(1, out_domain_turn_count) * 100, 2),
                                'turns': out_domain_turn_count}

        em_total = in_domain_em_total + out_domain_em_total
        f1_total = in_domain_f1_total + out_domain_f1_total
        turn_count = in_domain_turn_count + out_domain_turn_count
        scores["overall"] = {"recall": round(em_total / max(1, turn_count) * 100, 2),
                             'f1': round(f1_total / max(1, turn_count) * 100, 2),
                             'turns': turn_count}

        type_scores = OrderedDict()
        for tt in ["yes","no","unknown","extractive"]:
            type_scores[tt] = {}
            type_scores[tt]["recall"] = round(type_results[tt]['em_total'] / max(1, type_results[tt]['turn_count']) * 100, 2)
            type_scores[tt]['f1'] = round(type_results[tt]['f1_total'] / max(1, type_results[tt]['turn_count']) * 100, 2)
            type_scores[tt]['turns'] = type_results[tt]['turn_count']

        return scores,type_scores


class RobertaTokenizer(object):
    def __init__(self):
        try:
            import regex as re
            self.re = re
        except ImportError:
            raise ImportError('Please install regex with: pip install regex')
        self.pat = self.re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")
    
    def word_tokenizer(self, text):
        tokens = self.re.findall(self.pat, text)
        token_spans = []
        assert len(text) == sum(len(token) for token in tokens)
        start_index = 0
        for token in tokens:
            real_start_index = start_index if token[0] != ' ' else start_index + 1
            token_spans.append((real_start_index, start_index+len(token)))
            start_index += len(token)
        assert len(tokens) == len(token_spans)
        return tokens, token_spans

    def word_tokenizer_parallel(self, docs):
        # TODO real parallel implementation
        tokens, token_spans = [], []
        for doc in docs:
            tokens_doc, token_spans_doc = self.word_tokenizer(doc)
            tokens.append(tokens_doc)
            token_spans.append(token_spans_doc)
        return tokens, token_spans


class RunningAverage():
    """A simple class that maintains the running average of a quantity

    Example:
    ```
    loss_avg = RunningAverage()
    loss_avg.update(2)
    loss_avg.update(4)
    loss_avg() = 3
    ```
    """

    def __init__(self):
        self.steps = 0
        self.total = 0

    def update(self, val):
        self.total += val
        self.steps += 1

    def __call__(self):
        return self.total / float(self.steps)


class EMA():
    def __init__(self, model, decay):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}

    def register(self, shadow=None):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                if shadow is None:
                    self.shadow[name] = param.data.clone()
                else:
                    self.shadow[name] = shadow[name]

    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()

    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                self.backup[name] = param.data
                param.data = self.shadow[name]

    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}


def get_best_answer(output, instances, max_answer_len=20, null_score_diff_threshold=0.0):
    def _get_best_indexes(logits, n_best_size):
        """Get the n-best logits from a list."""
        index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)

        best_indexes = []
        for i in range(len(index_and_score)):
            if i >= n_best_size:
                break
            best_indexes.append(index_and_score[i][0])
        return best_indexes

    ground_answers = []
    qid_with_max_logits = {}
    qid_with_final_text = {}
    qid_with_no_logits = {}
    qid_with_yes_logits = {}
    qid_with_unk_logits = {}
    for i in range(len(instances)):
        instance = instances[i]
        ground_answers.append(instance['answer'])
        start_logits = output['start_logits'][i].tolist()
        end_logits = output['end_logits'][i].tolist()
        feature_unk_score = output['unk_logits'][i].tolist()[0] * 2
        feature_yes_score = output['yes_logits'][i].tolist()[0] * 2
        feature_no_score = output['no_logits'][i].tolist()[0] * 2
        start_indexes = _get_best_indexes(start_logits, n_best_size=20)
        end_indexes = _get_best_indexes(end_logits, n_best_size=20)
        max_start_index = -1
        max_end_index = -1
        max_logits = -100000000
        for start_index in start_indexes:
            for end_index in end_indexes:
                if start_index >= len(instance['tokens']):
                    continue
                if end_index >= len(instance['tokens']):
                    continue
                if start_index not in instance['token_to_orig_map']:
                    continue
                if end_index not in instance['token_to_orig_map']:
                    continue
                if end_index < start_index:
                    continue
                if not instance['token_is_max_context'].get(start_index, False):
                    continue
                length = end_index - start_index - 1
                if length > max_answer_len:
                    continue
                sum_logits = start_logits[start_index] + end_logits[end_index]
                if sum_logits > max_logits:
                    if not prediction_to_ori(start_index, end_index, instance).strip():
                        continue
                    max_logits = sum_logits
                    max_start_index = start_index
                    max_end_index = end_index
        final_text = ''
        if (max_start_index != -1 and max_end_index != -1):
            final_text = prediction_to_ori(max_start_index, max_end_index, instance)
        story_id, turn_id = instance["qid"].split("|")
        turn_id = int(turn_id)
        if (story_id, turn_id) in qid_with_max_logits and max_logits > qid_with_max_logits[(story_id, turn_id)]:
            qid_with_max_logits[(story_id, turn_id)] = max_logits
            qid_with_final_text[(story_id, turn_id)] = final_text
        if (story_id, turn_id) not in qid_with_max_logits:
            qid_with_max_logits[(story_id, turn_id)] = max_logits
            qid_with_final_text[(story_id, turn_id)] = final_text
        if (story_id, turn_id) not in qid_with_no_logits:
            qid_with_no_logits[(story_id, turn_id)] = feature_no_score
        if feature_no_score > qid_with_no_logits[(story_id, turn_id)]:
            qid_with_no_logits[(story_id, turn_id)] = feature_no_score
        if (story_id, turn_id) not in qid_with_yes_logits:
            qid_with_yes_logits[(story_id, turn_id)] = feature_yes_score
        if feature_yes_score > qid_with_yes_logits[(story_id, turn_id)]:
            qid_with_yes_logits[(story_id, turn_id)] = feature_yes_score
        if (story_id, turn_id) not in qid_with_unk_logits:
            qid_with_unk_logits[(story_id, turn_id)] = feature_unk_score
        if feature_unk_score > qid_with_unk_logits[(story_id, turn_id)]:
            qid_with_unk_logits[(story_id, turn_id)] = feature_unk_score
    result = {}
    for k in qid_with_max_logits:
        scores = [qid_with_max_logits[k], qid_with_no_logits[k], qid_with_yes_logits[k], qid_with_unk_logits[k]]
        max_val = max(scores)
        if max_val == qid_with_max_logits[k]:
            result[k] = qid_with_final_text[k]
        elif max_val == qid_with_unk_logits[k]:
            # result[k] = 'unknown'
            maxmax = max([qid_with_max_logits[k], qid_with_no_logits[k], qid_with_yes_logits[k]])
            if maxmax == qid_with_max_logits[k]:
                result[k] = qid_with_final_text[k]
            elif maxmax == qid_with_no_logits[k]:
                result[k] = 'no'
            else:
                result[k] = 'yes'
        elif max_val == qid_with_yes_logits[k]:
            result[k] = 'yes'
        else:
            result[k] = 'no'
    return result

def prediction_to_ori(start_index, end_index, instance):
    if start_index > 0:
        orig_doc_start = instance['token_to_orig_map'][start_index]
        orig_doc_end = instance['token_to_orig_map'][end_index]
        char_start_position = instance["context_token_spans"][orig_doc_start][0]
        char_end_position = instance["context_token_spans"][orig_doc_end][1]
        pred_answer = instance["context"][char_start_position:char_end_position]
        # token_answer = replace_u0120(''.join(instance['tokens'][start_index: end_index+1])).strip()
        # if pred_answer != token_answer and token_answer in pred_answer:
        #     print(pred_answer)
        #     print(token_answer)
        #     print()
        return pred_answer
    return ""

def replace_u0120(text):
    return text.replace('Ġ', ' ').replace('Ċ', '\n')
        

if __name__ == "__main__":
    # em means recall
    evaluator = CoQAEvaluator(os.path.join(args.data_folder, args.dev_filename))
    coqa_reader = CoQAReader(-1, evaluator=evaluator)
    dev_data = coqa_reader.read(args.data_folder + args.dev_filename, 'dev')
    # res_bert = None
    # with open('/root/fake_answer_out_dev.json', 'r', encoding='utf-8') as f:
    #     res_bert = json.load(f)
    #     res_bert = {(item['id'], item['turn_id']): item['answer'] for item in res_bert}
    res = {}
    res_save = []
    for instance in dev_data:
        id = instance['qid'].split('|')[0]
        turnid = int(instance['qid'].split('|')[1])
        qid = (id, turnid)
        res[qid] = instance['fake_answer']
        res_save.append({'answer': instance['fake_answer'], 'id': id, 'turn_id': turnid})
        # if CoQAEvaluator.normalize_answer(res[qid]) != CoQAEvaluator.normalize_answer(res_bert[qid]):
        #     print(qid, res[qid], ' --- ', res_bert[qid], instance['answer_start'], instance['answer_end'], instance['answer_type'])
    scores = evaluator.get_score(res)
    metrics_string = " ; ".join("{}: {:05.3f}".format(k, v) for k, v in scores.items())
    logger.info("- Eval metrics: " + metrics_string)
        